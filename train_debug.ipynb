{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Found cached data /home/old-ufo/dev/LearnedDetector/dataset/hpatches-sequences-release_a_train.pt\n",
      "# Found cached data /home/old-ufo/dev/LearnedDetector/dataset/hpatches-sequences-release_a_test.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from  scipy.ndimage import zoom as imzoom\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorch_sift import SIFTNet\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "USE_CUDA = False\n",
    "\n",
    "LOG_DIR = 'log_snaps'\n",
    "BASE_LR = 0.00000001\n",
    "from SpatialTransformer2D import SpatialTransformer2d\n",
    "from HardNet import HardNet\n",
    "hardnet = HardNet()\n",
    "checkpoint = torch.load('HardNetLib.pth')\n",
    "hardnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "class SparseImgRepresenter(nn.Module):\n",
    "    def __init__(self, \n",
    "             detector_net = None,\n",
    "             descriptor_net = None,    \n",
    "             use_cuda = False):\n",
    "        super(SparseImgRepresenter, self).__init__()\n",
    "        self.detector = detector_net;\n",
    "        self.descriptor = descriptor_net;\n",
    "        return\n",
    "    def forward(self, input_img, skip_desc = False):\n",
    "        aff_norm_patches, LAFs = self.detector(input_img)\n",
    "        if not skip_desc:\n",
    "            descs = self.descriptor(aff_norm_patches);\n",
    "            return aff_norm_patches, LAFs, descs\n",
    "        return aff_norm_patches, LAFs\n",
    "\n",
    "detnet = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=3, padding = 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=2,padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "ConvST_net = SpatialTransformer2d( num_input_channels = 1,\n",
    "                 num_ouput_channels = 32,\n",
    "                 feature_net = None,\n",
    "                 out_patch_size = 16,\n",
    "                 out_stride = 16,\n",
    "                 min_zoom = 0.9,\n",
    "                 max_zoom = 1.1,\n",
    "                 min_tilt = 0.9,\n",
    "                 max_tilt = 1.1,\n",
    "                 max_rot = 0.1,\n",
    "                 max_shift = 0.5,\n",
    "                 mrSize = 1.0, use_cuda = USE_CUDA)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal(m.weight.data)\n",
    "\n",
    "#SIRNet = SparseImgRepresenter(detector_net = ConvST_net, descriptor_net = hardnet)\n",
    "#aff_norm_patches, LAFs, descs = SIRNet(var_image_reshape)\n",
    "\n",
    "def distance_matrix_vector(anchor, positive):\n",
    "    \"\"\"Given batch of anchor descriptors and positive descriptors calculate distance matrix\"\"\"\n",
    "\n",
    "    d1_sq = torch.sum(anchor * anchor, dim=1)\n",
    "    d2_sq = torch.sum(positive * positive, dim=1)\n",
    "    eps = 1e-6\n",
    "    return torch.sqrt(torch.abs((d1_sq.expand(positive.size(0), anchor.size(0)) +\n",
    "                       torch.t(d2_sq.expand(anchor.size(0), positive.size(0)))\n",
    "                      - 2.0 * torch.bmm(positive.unsqueeze(0), torch.t(anchor).unsqueeze(0)).squeeze(0))+eps))\n",
    "def LAFs_to_H_frames(aff_pts, use_cuda = False):\n",
    "    H3_x = torch.Tensor([0, 0, 1 ]).unsqueeze(0).unsqueeze(0).expand_as(aff_pts[:,0:1,:]);\n",
    "    H3_x = torch.autograd.Variable(H3_x)\n",
    "    if use_cuda:\n",
    "        H3_x = H3_x.cuda()\n",
    "    return torch.cat([aff_pts, H3_x], dim = 1)\n",
    "def reproject_to_canonical_Frob_batched(LHF1_inv, LHF2, batch_size = 2, use_cuda = False):\n",
    "    out = torch.autograd.Variable(torch.zeros((LHF1_inv.size(0), LHF2.size(0))))\n",
    "    eye1 = torch.autograd.Variable(torch.eye(3), requires_grad = False)\n",
    "    if use_cuda:\n",
    "        out = out.cuda()\n",
    "        eye1 = eye1.cuda()\n",
    "    len1 = LHF1_inv.size(0)\n",
    "    len2 = LHF2.size(0)\n",
    "    n_batches = int(np.floor(len1 / batch_size) + 1);\n",
    "    for b_idx in range(n_batches):\n",
    "        #print b_idx\n",
    "        start = b_idx * batch_size;\n",
    "        fin = min((b_idx+1) * batch_size, len1)\n",
    "        current_bs = fin - start\n",
    "        if current_bs == 0:\n",
    "            break\n",
    "        resh1 = LHF1_inv[start:fin, :, :].unsqueeze(0).expand(len2,current_bs, 3, 3)\n",
    "        resh1 = resh1.contiguous().view(-1,3,3);\n",
    "        should_be_eyes = torch.bmm(resh1,\n",
    "                                   LHF2.unsqueeze(1).expand(len2,current_bs, 3,3).contiguous().view(-1,3,3))\n",
    "        out[start:fin, :] = torch.sum((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2, dim=1).sum(dim = 1).view(current_bs, len2)\n",
    "    return out\n",
    "\n",
    "def get_GT_correspondence_indexes(aff_pts1,aff_pts2, H1to2, dist_threshold = 4, use_cuda = False):\n",
    "    LHF2 = LAFs_to_H_frames(aff_pts2, use_cuda = use_cuda)\n",
    "    LHF2_reprojected_to_1 = torch.bmm(H1to2.expand_as(LHF2), LHF2);\n",
    "    LHF2_reprojected_to_1 = LHF2_reprojected_to_1 / LHF2_reprojected_to_1[:,2:,2:].expand_as(LHF2_reprojected_to_1);\n",
    "    just_centers1 = aff_pts1[:,:,2];\n",
    "    just_centers2_repr_to_1 = LHF2_reprojected_to_1[:,0:2,2];\n",
    "    dist  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1)\n",
    "    min_dist, idxs_in_2 = torch.min(dist,1)\n",
    "    plain_indxs_in1 = torch.autograd.Variable(torch.arange(0, idxs_in_2.size(0)),requires_grad = False)\n",
    "    if use_cuda:\n",
    "        plain_indxs_in1 = plain_indxs_in1.cuda()\n",
    "    mask =  min_dist <= dist_threshold\n",
    "    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n",
    "\n",
    "def get_GT_correspondence_indexes_Fro(aff_pts1,aff_pts2, H1to2, dist_threshold = 4, use_cuda = False):\n",
    "    LHF2 = LAFs_to_H_frames(aff_pts2, use_cuda = use_cuda)\n",
    "    LHF2_reprojected_to_1 = torch.bmm(H1to2.expand_as(LHF2), LHF2);\n",
    "    LHF2_reprojected_to_1 = LHF2_reprojected_to_1 / LHF2_reprojected_to_1[:,2:,2:].expand_as(LHF2_reprojected_to_1);\n",
    "    LHF1 = LAFs_to_H_frames(aff_pts1, use_cuda = False)\n",
    "    \n",
    "    LHF1_inv = torch.autograd.Variable(torch.zeros(LHF1.size()))\n",
    "    if use_cuda:\n",
    "        LHF1_inv = LHF1_inv.cuda()\n",
    "    for i in range(len(LHF1_inv)):\n",
    "        LHF1_inv[i,:,:] = LHF1[i,:,:].inverse()\n",
    "    frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_reprojected_to_1, batch_size = 2, use_cuda = use_cuda)\n",
    "    #just_centers1 = aff_pts1[:,:,2];\n",
    "    #just_centers2_repr_to_1 = LHF2_reprojected_to_1[:,0:2,2];\n",
    "    #dist  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1)\n",
    "    min_dist, idxs_in_2 = torch.min(frob_norm_dist,1)\n",
    "    plain_indxs_in1 = torch.autograd.Variable(torch.arange(0, idxs_in_2.size(0)))\n",
    "    if use_cuda:\n",
    "        plain_indxs_in1 = plain_indxs_in1.cuda()\n",
    "    mask =  min_dist <= dist_threshold\n",
    "    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n",
    "\n",
    "def adjust_learning_rate(optimizer):\n",
    "    \"\"\"Updates the learning rate given the learning rate decay.\n",
    "    The routine has been implemented according to the original Lua SGD optimizer\n",
    "    \"\"\"\n",
    "    n_triplets = 116*5.\n",
    "    n_epochs = 10.\n",
    "    for group in optimizer.param_groups:\n",
    "        if 'step' not in group:\n",
    "            group['step'] = 0.\n",
    "        else:\n",
    "            group['step'] += 1.\n",
    "        group['lr'] =  BASE_LR #*  .0 - float(group['step']) * float(1.0) / (n_triplets * float(n_epochs)))\n",
    "    return\n",
    "\n",
    "def create_optimizer(model, new_lr, wd):\n",
    "    # setup optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                          momentum=0.5, dampening=0.5,\n",
    "                          weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def create_loaders(load_random_triplets = False):\n",
    "\n",
    "    kwargs = {'num_workers': 2, 'pin_memory': True} if True else {}\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "    #        transforms.Normalize((args.mean_image,), (args.std_image,))])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            dset.HPatchesSeq('/home/old-ufo/dev/LearnedDetector/dataset/', 'a',\n",
    "                             train=True, transform=None, \n",
    "                             download=True), batch_size = 1,\n",
    "        shuffle = False, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            dset.HPatchesSeq('/home/old-ufo/dev/LearnedDetector/dataset/', 'a',\n",
    "                             train=False, transform=None, \n",
    "                             download=True), batch_size = 1,\n",
    "        shuffle = False, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = create_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.9670  1.3720  1.3227  ...   1.3188  1.0672  1.1342\n",
      " 1.1052  1.0927  1.0343  ...   1.2392  0.9956  1.0502\n",
      " 1.0429  1.4750  1.2408  ...   1.0685  0.9707  1.2900\n",
      "          ...             â‹±             ...          \n",
      " 0.9236  1.2108  1.0485  ...   1.0474  1.2338  1.0589\n",
      " 1.2955  1.7815  1.6110  ...   1.3607  1.2053  1.4766\n",
      " 1.1045  1.1967  1.3532  ...   1.1709  1.3722  1.5626\n",
      "[torch.DoubleTensor of size 34x125]\n",
      "\n",
      "torch.Size([34]) torch.Size([34]) \n",
      " 0.6738\n",
      " 0.6672\n",
      " 0.5441\n",
      " 0.6653\n",
      " 0.6209\n",
      " 0.5004\n",
      " 0.7529\n",
      " 0.5671\n",
      " 0.5135\n",
      " 0.8085\n",
      " 0.5102\n",
      " 0.7387\n",
      " 0.7693\n",
      " 0.4912\n",
      " 0.7294\n",
      " 0.7124\n",
      " 0.6740\n",
      " 0.6596\n",
      " 0.5502\n",
      " 0.6298\n",
      " 0.7058\n",
      " 0.6127\n",
      " 0.6672\n",
      " 0.6665\n",
      " 0.6601\n",
      " 0.6159\n",
      " 0.7656\n",
      " 0.7316\n",
      " 0.5576\n",
      " 0.4161\n",
      " 0.6304\n",
      " 0.5716\n",
      " 0.5736\n",
      " 0.6415\n",
      "[torch.DoubleTensor of size 34]\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "anc = torch.from_numpy(np.random.random((125,10)))\n",
    "pos = torch.from_numpy(np.random.random((34,10)))\n",
    "dm = distance_matrix_vector(anc,pos)\n",
    "print dm\n",
    "min_dist, idxs_in_2 = torch.min(dm,1)\n",
    "print min_dist.shape, idxs_in_2.shape, min_dist\n",
    "print np.sum(np.isnan(dm.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "SIRNet = SparseImgRepresenter(detector_net = ConvST_net, descriptor_net = SIFTNet(patch_size = 16, do_cuda = USE_CUDA))\n",
    "SIRNet.detector.apply(weights_init)\n",
    "\n",
    "model = SIRNet\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer1 = create_optimizer(model, BASE_LR, 5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model, optimizer, epoch, cuda = True):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    log_interval = 1\n",
    "    spatial_only = True\n",
    "    pbar = enumerate(train_loader)\n",
    "    for batch_idx, data in pbar:\n",
    "        print 'Batch idx', batch_idx\n",
    "        #print model.detector.shift_net[0].weight.data.cpu().numpy()\n",
    "        img1, img2, H  = data\n",
    "        #if np.abs(np.sum(H.numpy()) - 3.0) > 0.01:\n",
    "        #    continue\n",
    "        H = H.squeeze(0)\n",
    "        img1 = img1.float().squeeze(0)\n",
    "        img1 = img1 - img1.mean()\n",
    "        img1 = img1 / 50.#(img1.std() + 1e-8)\n",
    "        img2 = img2.float().squeeze(0)\n",
    "        img2 = img2 - img2.mean()\n",
    "        img2 = img2 / 50.#(img2.std() + 1e-8)\n",
    "        if cuda:\n",
    "            img1, img2, H = img1.cuda(), img2.cuda(), H.cuda()\n",
    "        img1, img2, H = Variable(img1), Variable(img2), Variable(H)\n",
    "        aff_norm_patches1, LAFs1 = model(img1, skip_desc = True)\n",
    "        aff_norm_patches2, LAFs2 = model(img2, skip_desc = True)\n",
    "        fro_dists, idxs_in1, idxs_in2 = get_GT_correspondence_indexes_Fro(LAFs1, LAFs2, H, dist_threshold = 1.0, use_cuda = cuda);\n",
    "        if  len(fro_dists.size()) == 0:\n",
    "            optimizer.zero_grad()\n",
    "            print 'skip'\n",
    "            continue\n",
    "        loss = fro_dists.mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #adjust_learning_rate(optimizer)\n",
    "        print epoch,batch_idx, loss.data.cpu().numpy()[0]\n",
    "\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()},\n",
    "               '{}/checkpoint_{}.pth'.format(LOG_DIR, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Batch idx 0\n",
      "0 0 0.0464067\n",
      "Batch idx 1\n",
      "0 1 0.0472269\n",
      "Batch idx 2\n",
      "0 2 0.0476021\n",
      "Batch idx 3\n",
      "0 3 0.0489703\n",
      "Batch idx 4\n",
      "0 4 0.0498334\n",
      "Batch idx 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    r = index_queue.get()\n",
      "    racquire()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "KeyboardInterrupt\n",
      "    return recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5b555985049c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-3d3fcc16c01a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, optimizer, epoch, cuda)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfro_dists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#adjust_learning_rate(optimizer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \"\"\"\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 98\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start = 0\n",
    "end = 10\n",
    "for epoch in range(start, end):\n",
    "    print 'epoch', epoch\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "    train(train_loader, model, optimizer1, epoch, cuda = USE_CUDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuda = False\n",
    "model.train()\n",
    "log_interval = 1\n",
    "spatial_only = True\n",
    "pbar = enumerate(train_loader)\n",
    "for batch_idx, data in pbar:\n",
    "    #print model.detector.shift_net[0].weight.data.cpu().numpy()\n",
    "    img1, img2, H  = data\n",
    "    #if np.abs(np.sum(H.numpy()) - 3.0) > 0.01:\n",
    "    #    continue\n",
    "    H = H.squeeze(0)\n",
    "    img1 = img1.float().squeeze(0)\n",
    "    img1 = img1 - img1.mean()\n",
    "    img1 = img1 / 50.#(img1.std() + 1e-8)\n",
    "    img2 = img2.float().squeeze(0)\n",
    "    img2 = img2 - img2.mean()\n",
    "    img2 = img2 / 50.#(img2.std() + 1e-8)\n",
    "    if cuda:\n",
    "        img1, img2, H = img1.cuda(), img2.cuda(), H.cuda()\n",
    "    img1, img2, H = Variable(img1), Variable(img2), Variable(H)\n",
    "    aff_norm_patches1, LAFs1 = model(img1, skip_desc = True)\n",
    "    aff_norm_patches2, LAFs2 = model(img2, skip_desc = True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LHF2 = LAFs_to_H_frames(LAFs2, use_cuda = False)\n",
    "LHF2_reprojected_to_1 = torch.bmm(H.expand_as(LHF2), LHF2);\n",
    "LHF2_reprojected_to_1 = LHF2_reprojected_to_1 / LHF2_reprojected_to_1[:,2:,2:].expand_as(LHF2_reprojected_to_1);\n",
    "LHF1 = LAFs_to_H_frames(LAFs1, use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LHF1_inv = torch.autograd.Variable(torch.zeros(LHF1.size()))\n",
    "if False:\n",
    "    LHF1_inv = LHF1_inv.cuda()\n",
    "for i in range(len(LHF1_inv)):\n",
    "    LHF1_inv[i,:,:] = LHF1[i,:,:].inverse()\n",
    "#should_be_eye = torch.bmm(LHF1_inv, LHF2_reprojected_to_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "should_be_eye = torch.bmm(LHF1_inv.expand(), LHF2_reprojected_to_1)\n",
    "print should_be_eye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9988  0.0000 -0.0022\n",
      "-0.0000  0.9993 -0.0073\n",
      " 0.0000  0.0000  1.0000\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print should_be_eye[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A2_1 =  LHF2_reprojected_to_1[0,:,:]\n",
    "A1 =  LHF1[0,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "should_be_eye = torch.matmul(A1.inverse(), A2_1)\n",
    "frob_norm_err = torch.nn.MSELoss()(should_be_eye, torch.autograd.Variable(torch.eye(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-06 *\n",
       "  6.6001\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frob_norm_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "just_centers1 = LAFs1[:,:,2];\n",
    "just_centers2_repr_to_1 = LHF2_reprojected_to_1[:,0:2,2];\n",
    "print 'min, max AP1', just_centers1.data.cpu().numpy().min(), just_centers1.data.cpu().numpy().max()\n",
    "print 'min, max AP2', just_centers2_repr_to_1.data.cpu().numpy().min(), just_centers2_repr_to_1.data.cpu().numpy().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dist  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 440.2994\n",
      "  24.0781\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      " 440.2048\n",
      "  24.1159\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print just_centers2_repr_to_1[59,:]\n",
    "print just_centers1[59,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1768\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def distance_matrix_vector(anchor, positive):\n",
    "    \"\"\"Given batch of anchor descriptors and positive descriptors calculate distance matrix\"\"\"\n",
    "\n",
    "    d1_sq = torch.sum(anchor * anchor, dim=1)\n",
    "    d2_sq = torch.sum(positive * positive, dim=1)\n",
    "    eps = 1e-6\n",
    "    return torch.sqrt(torch.abs((d1_sq.expand(positive.size(0), anchor.size(0)) +\n",
    "                       torch.t(d2_sq.expand(anchor.size(0), positive.size(0)))\n",
    "                      - 2.0 * torch.bmm(positive.unsqueeze(0), torch.t(anchor).unsqueeze(0)).squeeze(0))+eps))\n",
    "print distance_matrix_vector(just_centers2_repr_to_1[59:60,:],just_centers1[59:60,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_np = dist.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 59, 215, 277, 285, 364, 398, 407, 472, 496, 747, 903, 910, 955,\n",
      "       960, 964]), array([ 59, 215, 277, 285, 364, 398, 407, 472, 496, 747, 903, 910, 955,\n",
      "       960, 964]))\n"
     ]
    }
   ],
   "source": [
    "print np.where(np.isnan(dist_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatialTransformer2d (\n",
      "  (spatial_transformer_feature_net): Sequential (\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU ()\n",
      "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): ReLU ()\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): ReLU ()\n",
      "  )\n",
      "  (psi_net): Sequential (\n",
      "    (0): Conv2d(32, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): Tanh ()\n",
      "  )\n",
      "  (theta_net): Sequential (\n",
      "    (0): Conv2d(32, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): Tanh ()\n",
      "  )\n",
      "  (shift_net): Sequential (\n",
      "    (0): Conv2d(32, 2, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): Tanh ()\n",
      "  )\n",
      "  (iso_scale_net): Sequential (\n",
      "    (0): Conv2d(32, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): Tanh ()\n",
      "  )\n",
      "  (horizontal_tilt_net): Sequential (\n",
      "    (0): Conv2d(32, 1, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): Tanh ()\n",
      "  )\n",
      ")\n",
      "[[[[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  ..., \n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]]\n",
      "\n",
      "\n",
      " [[[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  ..., \n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]\n",
      "\n",
      "  [[ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]\n",
      "   [ nan  nan  nan  nan]]]]\n"
     ]
    }
   ],
   "source": [
    "print model.detector\n",
    "#print np.sum(np.isnan(model.detector.shift_net[0].weight.data.cpu().numpy()))\n",
    "print model.detector.shift_net[0].weight.data.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
